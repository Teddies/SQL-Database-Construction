1. Basic information
Team number (e.g., 01) : 10
#1 Student ID : 54369161
#1 Student Name : Lingxi Meng
#2 Student ID : 49047898
#2 Student Name : Di Wu
OS (bit) : 64-bit Ubuntu (Linux) / macOS 10.12.6(64bit)
gcc version : (Ubuntu 6.3.0-12ubuntu2) 6.3.0 20170406


2. Catalog information about Index
- Show your catalog information about an index (tables, columns). 
In Relation manager, we encapsulate the functions of index manager, and let the relation manager handles both record based heap file and B+ tree file, and keep the index file synchronous with record based file. In order to handle it, we first register each index file in 'Tables' table every time creating a index based file in the following format.
					   -------------------------------------------------------------
					   |    id    |     indexTablesName     |    indexFilesName    |     	
					   -------------------------------------------------------------
The indexFilesName is composed in this format : TableName_AttributeName.idx
And there is no registration in "Columns" table

3. Block Nested Loop Join (If you have implemented this feature)
- Describe how your block nested loop join works (especially, how you manage the given buffers.)

Given a buffer size, i.e. block capacity (number of pages * page size), we use a while loop to retrieve records from left table until the records fill all buffer. And we build a hash map through buffer' all records. After that, we retrieve records from right table one by one to do a look up in this hash map. Once joining the right record, we put it into the output buffer (1 page, 4096 bytes). And when buffer is full, we flush the whole output buffer into disk and prepare for further join. Through this line, we retrieve buffer size byte each time retrieving left table.

                                                      number of pages * page size byte
					   -------------------------------------------------------------
					   |                                                           | 
                                           |  retrieve until buffer is full                            | 
                leftTable ---------------------------------->                                          | 
                                           |                                                           |                   |---------------------------| flush to disk
                                           |                build a in memory hash map                 | 		   |         		   --------------------->
                                           |                                                           |   join		   |	output buffer          |
                                           |                                                           |-----------------------> 		       |
					   |                                                           |                   ----------------------------- 	
					   -------------------------------------------------------------
					        /\
						|
						|
						| do a look up each record
						|
						|
				            rightTable

4. Index Nested Loop Join (If you have implemented this feature)
- Describe how your grace hash join works.

We loop through left index file to get each attribute value, and for each attribute, we try to match it in the right index file.
Once matching, we put it into data for printing.

5. Grace Hash Join (If you have implemented this feature)
- Describe how your grace hash join works (especially, in-memory structure).

First of all, partition phrase. We malloc an input buffer (number of partitions * PAGE_SIZE) (buildHash function). For R or S table, we retrieve each record into the input buffer's matched page using a self-defined hash function. Eg. if a record's hash number is k, we put it into k'th page in the input buffer. If one page in the input buffer is full, we write the whole page into a partition file. Until exhausting both R and S tables, we get a two batches of partition files (each file's records have the same hashcode).
Secondly, build phrase. We use buildhash function to retrieve records from R table's partition files and build a in-memory hash map. 
Thirdly, probe phrase. After building hash map, we read records from S table's partition file one by one, and use a lookUpHash function to match each record in S's partition file. Every time matching, we put the record into a output vector (in outputRes function).

6. Aggregation
- Describe how your aggregation (basic, group-based hash) works.
For basic:
First use different function to handle different AggregateOp(min, max, count, sum, average). For instance, we consistently get the next tuple using input iterator acquired in constructor until exhausted. And each time we refresh our min value. Finally, we write the min value and its former null indicator in data to return. Similar implementation way for other four functions.

For group-based hash:
We use a map structure to store the fresh information : key is the value of left attribute, and value is the fresh target value of the right attribute. We implement different functions in order to call in getNextTuple function for different AggregateOp.
For example, in group sum function, we refresh the pair(for old key) or insert pair(for new key) when getting tuple in input iterator in while loop.
The map structure is like this format: <valueInLeafKey, currentCount(rightKey)>

7. Implementation Detail
- Have you added your own source file (.cc or .h)?
No.
- Have you implemented any optional features? Then, describe them here.
Yes. We implemented optional aggregate and Grace Hash Join. 
For optional aggregate, we use three different map (<int, float>, <float, float> and <string, float>) and vector to build the key-value pair for each getNextTuple in the input iterator. And fresh the map each time acquiring a new record. For example, if aggregateOp is MAX, we add a getGroupMax function to do it. Each time getting a record, we find if the value of aggAttr exists in map: if not we put the new pair into map; if exist, we compare the value of gAttr to fresh the max value. After loop through the getNextTuple in input iterator, we finally get the banch max value.for each value of aggAttr.
For GHJ, we implemented it through the way of professor's lecture. There are three step: partition phrase (partition R and S tables into small partition file using a self-defined function), build phrase (build a in-memory hash table with given size),probe phrase (loop through S table one by one, flushing into output buffer once matching). The further details of GHJ can be found in section 5.
 
- Other implementation details:


6. Other (optional)
- Freely use this section to tell us about things that are related to the project 4, but not related to the other sections (optional)

